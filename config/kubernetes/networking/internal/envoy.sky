# DO NOT EDIT: http://go/vendor-skycfg
load(
    "config/kubernetes/core/container.sky",
    "container",
    "container_port",
)
load("config/kubernetes/core/env_var.sky", "container_env_vars")
load("config/kubernetes/core/probe.sky", "http_probe", "probes")
load("config/kubernetes/core/volume.sky", "mount_host_volume")
load(
    "config/kubernetes/helpers/security.sky",
    "mount_credentials_proxy",
    "use_credentials_proxy",
    "allow_ulimit_management",
)
load("config/kubernetes/plugins/compose.sky", "compose_plugins")
load(
    "config/kubernetes/networking/internal/config/envoy-config-srv-config.sky",
    "CDS_PORT",
    "DEBUG_PORT",
    "ENVOY_EGRESS_SOCKETS_DIR",
    "ENVOY_LOGS_DIR",
    "ENVOY_OUTBOUND_UNIX_SOCKETS_DIR",
    "ENVOY_RUNTIME_DIR",
    "ENVOY_SYSTEM_FLAGS_PATH",
    "LDS_PORT",
    "UNPRIVILEGED_ENVOY_PORT",
)
load(
    "config/kubernetes/networking/internal/helpers.sky",
    "host_mount_stripe_cas",
    "mount_splunk_forwarded_basedir",
    "pod_mount",
)
load(
    "config/kubernetes/networking/internal/config/envoy-sidecar-config.sky",
    "generate_cli_args",
    "generate_envoy_bootstrap_opts",
)
load(
    "config/kubernetes/networking/internal/envoy-config-srv.sky",
    "envoy_sidecar_config",
)
load("config/kubernetes/helpers/constants.sky", "ENVOY_SIDECAR_NAME")

ENVOYCONTROL_V2_CIRCUIT_BREAKER_LIMITS = 4096

# When trying to test a digest from a non-master branch, use stripe-qa instead of stripe in the URI
_CONTAINER_IMAGES = {
    # Prod sha256 digest
    # CI Build  : https://cibot.corp.stripe.com/builds/gocode-containers--bui_MYlGt55CcnCrJE
    # Git Commit: d3fc8583e8d4d86ece969e93a0ef4c4842de89c2
    "prod": "containers.global.prod.stripe.io/stripe/traffic/envoy-sidecar@sha256:4cab1bd3c09e5fe60be29ad85d84d3f8bed22f74e40d1ba7237dc7a751d4a911",

    # Preprod sha256 digest
    # CI Build  : https://cibot.corp.stripe.com/builds/gocode-containers--bui_MYlGt55CcnCrJE
    # Git Commit: d3fc8583e8d4d86ece969e93a0ef4c4842de89c2
    "preprod": "containers.global.prod.stripe.io/stripe/traffic/envoy-sidecar@sha256:4cab1bd3c09e5fe60be29ad85d84d3f8bed22f74e40d1ba7237dc7a751d4a911",

    # QA sha256 digest
    # CI Build  : https://cibot.corp.stripe.com/builds/gocode-containers--bui_MYlGt55CcnCrJE
    # Git Commit: d3fc8583e8d4d86ece969e93a0ef4c4842de89c2
    "qa": "containers.global.prod.stripe.io/stripe/traffic/envoy-sidecar@sha256:4cab1bd3c09e5fe60be29ad85d84d3f8bed22f74e40d1ba7237dc7a751d4a911",
}

_OPEN_FILES_LIMIT = 262144

def envoy_sidecar(ctx, namespace, config_target, user_overrides):
    name = ENVOY_SIDECAR_NAME
    return compose_plugins(
        envoy_container(
            ctx,
            name,
            config_target,
            user_overrides,
            envoy_id = "//$(STRIPE_NODE_NAME)/$(STRIPE_POD_NAMESPACE)/$(STRIPE_POD_NAME)",
            namespace = namespace,
        ),
        # envoy-sidecar provides an SDS endpoint w/ creds proxy as the backend
        use_credentials_proxy(),
        mount_credentials_proxy(container_name = name),
        # envoy-sidecar must write envoy-sds.sock to some path in /tmp:
        # - then write a bootstrap config with a static cluster whose socket
        #   address points at envoy-sds.sock
        # - this configures the Envoy spawned by this sidecar to use
        #   envoy-sds.sock as the SDS entry point for fetching a cert from
        #   credentials-proxy
        pod_mount(name, "/tmp"),
        # TODO(xyu): does envoy-sidecar need this?
        host_mount_stripe_cas(name),
        # Store the state for toggling fail-all-healthchecks via envoy admin endpoint
        pod_mount(name, ENVOY_RUNTIME_DIR),
        # XXX These /var/run/envoy/* dirpaths are hard-coded by envoy-config-srv
        # and configure envoy to write unix socket files to these paths
        # NB(xyu): /var/run/envoy/egress contains sockets for listeners that
        # forward requests to the egress proxies
        pod_mount(name, ENVOY_EGRESS_SOCKETS_DIR),
        # NB(xyu) /var/run/envoy/outbound contains sockets corresponding to unix
        # listeners for each outbound cluster - this is only used by "edge proxy"
        # envoys (e.g. on intfe or clusterfe). Include this mount for compatibility.
        pod_mount(name, ENVOY_OUTBOUND_UNIX_SOCKETS_DIR),
        mount_splunk_forwarded_basedir(container_name = name),
        # Need this for:
        # - /pay/conf/ec2_placement_availability_zone - we only know the AZ at
        #   runtime, and we want the bootstrap config emitted by envoy-sidecar
        #   to have the correct node.locality.zone property
        mount_host_volume(
            "/pay/conf",
            container_name = name,
            mount_args = {"read_only": True},
            volume_args = {
                "type": "Directory",
                "reason": "envoy-sidecar needs to access /pay/conf/ec2_placement_availability_zone",
            },
        ),
    )

# XXX(xyu): This is how we currently identify an envoy instance, however these
# fields are largely unused -- we merely log them as attributes of the
# requesting Envoy instance. The relevant bits are injected into
# envoy-config-srv's configuration -- those are the ones that matter for
# decisions like az-aware routing, filtering the service graph reachability etc.
#
# Nevertheless, here's what this looks like:
#
#   id: qa-hensontestbox--0874096813d04c033.northwest.stripe.io  # msp: "//<hostname>/<namespace>/<podname>"
#   cluster: hensontestbox                                       # msp: <namespace>
#   locality:
#     region: northwest                                          # msp: stripe-cluster (not an actual EC2 region)
#     zone: us-west-2c                                           # msp: TODO inject
def envoy_container(ctx, self, config_target, user_overrides, *, envoy_id, namespace):
    probe_port = 12345

    cli_args = generate_cli_args(config_target, user_overrides)

    cfg_json = envoy_sidecar_config(
        config_target,
        user_overrides,
        envoy_id = envoy_id,
        cluster = namespace,
        # NB(xyu): We don't have access to AZ placement info at deploy-time:
        # - henson-agent evaluates the skyconfig to k8s protos and then submits
        #   them to the Kube API
        # - It is then up to the Kube API to schedule onto some node in some AZ
        # This means that the only way to get AZ info is at pod runtime.
        # This is a placeholder value that we expect to be overwritten when
        # envoy-sidecar actually runs and reads AZ info out of /pay/conf
        zone = "fixme-shared-msp-fake-az",
    )

    container_image = _CONTAINER_IMAGES.get(config_target.env)
    if container_image == None:
        fail("Unexpected environment %s, could not find corresponding envoy-sidecar container image" % config_target.env)

    command = [
        "/bin/envoy-sidecar",
        "--http-address=0.0.0.0:%s" % probe_port,
        "--external-ip=$(KUBERNETES_POD_IP)",
        "--envoy-sidecar-config-json=" + json.marshal(cfg_json),
        "--rlimit-no-file=%d" % _OPEN_FILES_LIMIT,
    ]

    if cli_args["envoy_concurrency"] != None:
        command.append("--concurrency=%s" % cli_args["envoy_concurrency"])

    return container(
        name = self,
        image = container_image,
        command = command,
        sidecar_service = self,
        plugins = [
            container_env_vars(
                from_fields = {"KUBERNETES_POD_IP": "status.podIP"},
            ),
            container_port(
                UNPRIVILEGED_ENVOY_PORT,
                container_name = self,
                port_name = "mtls-ingress",
            ),
            probes(
                readiness = http_probe(
                    ctx,
                    port = probe_port,
                    path = "/msp-infra-ready",
                ),
                # add a startup probe to give envoy-sidecar time to initialize before the liveness probe kicks in
                startup = http_probe(
                    ctx,
                    port = probe_port,
                    path = "/msp-infra-live",
                    failureThreshold = 30,
                    periodSeconds = 10,
                ),
            ),
            allow_ulimit_management(container_name = self),
        ],
    )
